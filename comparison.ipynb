{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the DistilBERT model performance with other models such as TF-IDF and Word2Vec\n",
    "\n",
    "### Load the dataset and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "df_train = pd.DataFrame(dataset['train']) # Use the train split for training\n",
    "df_test = pd.DataFrame(dataset['test']) # Use the test split for evaluation\n",
    "\n",
    "df_train = df_train[['text', 'label']] # Select only the text and label columns\n",
    "df_train.columns = ['review', 'sentiment'] # Rename columns to match the required format\n",
    "\n",
    "df_test = df_test[['text', 'label']] # Select only the text and label columns\n",
    "df_test.columns = ['review', 'sentiment'] # Rename columns to match the required format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize using regular expression and nltk stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ashish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation\n",
    "    tokens = word_tokenize(text) # Tokenize the text\n",
    "    tokens = [word for word in tokens if word not in stop_words] # Remove stop words\n",
    "    return tokens, ' '.join(tokens)\n",
    "\n",
    "df_train['tokens'], df_train['cleaned_text'] = zip(*df_train['review'].apply(preprocess_text))\n",
    "df_test['tokens'], df_test['cleaned_text'] = zip(*df_test['review'].apply(preprocess_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rented, curious, yellow, video, store, contro...</td>\n",
       "      <td>rented curious yellow video store controversy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "      <td>[curious, yellow, risible, pretentious, steami...</td>\n",
       "      <td>curious yellow risible pretentious steaming pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[avoid, making, type, film, future, film, inte...</td>\n",
       "      <td>avoid making type film future film interesting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>[film, probably, inspired, godard, masculin, f...</td>\n",
       "      <td>film probably inspired godard masculin féminin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "      <td>[oh, brother, hearing, ridiculous, film, umpte...</td>\n",
       "      <td>oh brother hearing ridiculous film umpteen yea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...          0   \n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...          0   \n",
       "2  If only to avoid making this type of film in t...          0   \n",
       "3  This film was probably inspired by Godard's Ma...          0   \n",
       "4  Oh, brother...after hearing about this ridicul...          0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [rented, curious, yellow, video, store, contro...   \n",
       "1  [curious, yellow, risible, pretentious, steami...   \n",
       "2  [avoid, making, type, film, future, film, inte...   \n",
       "3  [film, probably, inspired, godard, masculin, f...   \n",
       "4  [oh, brother, hearing, ridiculous, film, umpte...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  rented curious yellow video store controversy ...  \n",
       "1  curious yellow risible pretentious steaming pi...  \n",
       "2  avoid making type film future film interesting...  \n",
       "3  film probably inspired godard masculin féminin...  \n",
       "4  oh brother hearing ridiculous film umpteen yea...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[love, sci, fi, willing, put, lot, sci, fi, mo...</td>\n",
       "      <td>love sci fi willing put lot sci fi movies tv u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worth the entertainment value of a rental, esp...</td>\n",
       "      <td>0</td>\n",
       "      <td>[worth, entertainment, value, rental, especial...</td>\n",
       "      <td>worth entertainment value rental especially li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[totally, average, film, semi, alright, action...</td>\n",
       "      <td>totally average film semi alright action seque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STAR RATING: ***** Saturday Night **** Friday ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[star, rating, saturday, night, friday, night,...</td>\n",
       "      <td>star rating saturday night friday night friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First off let me say, If you haven't enjoyed a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[first, let, say, enjoyed, van, damme, movie, ...</td>\n",
       "      <td>first let say enjoyed van damme movie since bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I love sci-fi and am willing to put up with a ...          0   \n",
       "1  Worth the entertainment value of a rental, esp...          0   \n",
       "2  its a totally average film with a few semi-alr...          0   \n",
       "3  STAR RATING: ***** Saturday Night **** Friday ...          0   \n",
       "4  First off let me say, If you haven't enjoyed a...          0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [love, sci, fi, willing, put, lot, sci, fi, mo...   \n",
       "1  [worth, entertainment, value, rental, especial...   \n",
       "2  [totally, average, film, semi, alright, action...   \n",
       "3  [star, rating, saturday, night, friday, night,...   \n",
       "4  [first, let, say, enjoyed, van, damme, movie, ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  love sci fi willing put lot sci fi movies tv u...  \n",
       "1  worth entertainment value rental especially li...  \n",
       "2  totally average film semi alright action seque...  \n",
       "3  star rating saturday night friday night friday...  \n",
       "4  first let say enjoyed van damme movie since bl...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup TF-IDF Vectorizer for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(df_train['cleaned_text'])\n",
    "X_tfidf_test = tfidf_vectorizer.transform(df_test['cleaned_text'])\n",
    "\n",
    "y_train = df_train['sentiment']\n",
    "y_test = df_test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Word2Vec Embedding Model for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "w2v_model = Word2Vec(sentences=df_train['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_w2v_embedding(tokens, model, vector_size=100):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "# Create embeddings for train and test\n",
    "X_w2v_train = np.array([get_w2v_embedding(tokens, w2v_model, 100) for tokens in df_train['tokens']])\n",
    "X_w2v_test = np.array([get_w2v_embedding(tokens, w2v_model, 100) for tokens in df_test['tokens']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the TF-IDF and Word2Vec Model and print their accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model Accuracy: 0.88296\n",
      "Classification Report (TF-IDF):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     12500\n",
      "           1       0.88      0.88      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n",
      "Word2Vec Model Accuracy: 0.81048\n",
      "Classification Report (Word2Vec):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81     12500\n",
      "           1       0.81      0.81      0.81     12500\n",
      "\n",
      "    accuracy                           0.81     25000\n",
      "   macro avg       0.81      0.81      0.81     25000\n",
      "weighted avg       0.81      0.81      0.81     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# TF-IDF Model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_tfidf_train, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_tfidf_test)\n",
    "print(\"TF-IDF Model Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "# Word2Vec Model\n",
    "clf_w2v = LogisticRegression(max_iter=1000)\n",
    "clf_w2v.fit(X_w2v_train, y_train)\n",
    "y_pred_w2v = clf_w2v.predict(X_w2v_test)\n",
    "print(\"Word2Vec Model Accuracy:\", accuracy_score(y_test, y_pred_w2v))\n",
    "print(\"Classification Report (Word2Vec):\\n\", classification_report(y_test, y_pred_w2v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
